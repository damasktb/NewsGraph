During the requirements gathering process, four distinct components of the system were identified which form a pipeline of execution; \textit{Article Retrieval}, \textit{Keyword Extraction}, \textit{Graph Building} and \textit{Map Drawing}. Crucially, all key areas of functionality within components are decoupled by design, allowing both for flexible extensibility and for alternative implementations to be tested directly against each other without requiring changes to the other modules.

This chapter decomposes each of the four components in order, and provides a detailed overview of their design and implementation, including a discussion of significant challenges and successes which arose during development.

\section{Code Reuse}
The system was developed in Python 2.7 and JavaScript 1.7 using various open-source libraries and APIs. The most notable are detailed below and discussed in context in the following sections.
\begin{itemize}[itemsep=0.1em]
	\item\textbf{FeedParser}\footnote{\url{http://pythonhosted.org/feedparser}}: A Python module for downloading and parsing RSS feeds.
	\item\textbf{Goose Extractor}\footnote{\url{https://pypi.python.org/pypi/goose-extractor/}} A Python library for downloading and extracting cleaned text and metadata from online articles. 
	\item\textbf{lxml}\footnote{\url{http://lxml.de}}: A Python library for generating, parsing and manipulating XML and HTML.
	\item\textbf{NLTK (The Natural Language Toolkit)}\footnote{\url{http://www.nltk.org}}: A Python library for natural language processing and analysis.
	\item\textbf{Google Knowledge Graph Search}\footnote{\url{https://developers.google.com/knowledge-graph}}: The API for Google's knowledge base, which returns structured semantic search results.
	\item\textbf{D3.js}\footnote{\url{http://d3js.org}} (v2): A JavaScript library for creating and manipulating interactive data-driven web visualisations.
\end{itemize}

\section{Article Retrieval}

The first stage of article retrieval is the parsing of RSS feeds, in order for article URLs and metadata to be extracted. This component of the system is substantially smaller than the remaining three, as it is simply a preprocessing task.

The Python library FeedParser was used for parsing, as at the time of writing it provided the best support for RSS 0.9x, 1.0 and 2.0. The parsing process extracts a link, channel name, and the parsed publish date from every article in the feed, but it will also attempt to extract the author name if the \texttt{author} attribute is found.

Once the feed data has been extracted, it is used to construct an instance of \texttt{ArticleCollection}, which acts a wrapper around the contents of one or more feeds and provides the mechanism necessary to perform corpus-wide queries. The \texttt{Article} class encapsulates the functions for computing article-specific terms such as term-frequency for tf-idf, and the term-weighting component of tf-pdf. 


\section{Keyword Extraction} \label{sec:keys}

The keyword extraction stage begins with the process of tokenising the body text of every article. Tokenisation here has three substages, all of which were implemented using NLTK (The Natural Language Toolkit for Python), and which are as follows (see Figure \ref{fig:tokenisation}):

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=\textwidth]{img/implementation/Tokenisation.pdf}
	\caption{Stepping through the tokenisation process}
	\label{fig:tokenisation}
\end{figure}

\begin{enumerate}[itemsep=0.3em]
	\item\textbf{Sentence segmentation:} Split the text into a list of sentences and remove sentence punctuation. This process is important, to prevent phrases being mistakenly identified across clause boundaries. It is also a non-trivial problem due to the ambiguity of the full-stop in English language texts \citep{TokenisationAndSentenceSegmentation}.
	\item\textbf{Tokenisation:} Split each sentence into a list of individual words and remove both whitespace and any remaining clause punctuation.
	\item\textbf{Part-of-Speech (POS) tagging:} Categorise each token according to its lexical class, e.g. adjective (\textit{JJ} in NLTK). This is more of an extension to the tokenisation process than strictly a part of it, but it is still necessary for subsequent processing.
\end{enumerate}

Once the raw article bodies and titles have been tokenised and tagged, the next subproblem is identifying words and phrases of importance within articles. These keywords will form our set of candidate metro lines in the next stage of the pipeline.

\subsection{Named Entity Recognition}

The second half of the keyword extraction process is exclusively concerned with named entities within articles. This is because, intuitively, the names of people, places, events, companies and other \textit{things} form a set of strong candidate keywords. 

Keyword extraction from named entities has been empirically shown to outperform other methods for extraction such as using only noun phrases \citep{EventTracking}. Additionally, restricting the candidates to entities alone reduces the search space by an order of magnitude when using frequency-based methods for keyword extraction, and bypasses the need for other natural language processing tasks such as stop-word removal and lemmatisation.

After tokens have been POS tagged, named-entity chunking can be performed, again using NLTK. This process groups tokens into contiguous and non-overlapping \textit{chunks}, where each chunk is a named entity; typically a proper noun or some other noun phrase. It is possible for chunks to contain other chunks (consider the chunk `Bank of England', which contains the chunk `England'), but this is typically undesirable for entity recognition, where we desire specificity. Consequentially, after chunking the tokens, we flatten the chunk structure so chunks cannot be any further decomposed.

The result of this process is a list of named-entity chunks, each of which will be an eventual candidate for becoming a metro line. However, the chunks require some further processing before we can determine whether or not they are significant keywords within the source text.

\subsubsection{Substring Matching}
It is common stylistic practice in all forms of journalism to refer to the subjects of articles by their surnames. However, to avoid any confusion, the full names of those mentioned will often appear in the title or first few paragraphs of the article. If keyword strength is determined by frequency, then regardless of whether we use tf-idf or tf-pdf, every occurrence of an entity must refer to that entity by the same name; preferably the most specific, which is typically the longest.

During this stage of disambiguation, we only consider entities which are mentioned twice or more in the source text as candidates. This is because the chunking process can occasionally produce false positives by combining unrelated adjectives with valid chunks, but the likelihood of the same false positive being produced multiple times within the same article is much lower.

To begin the process of substring matching, let $P$ denote the set of entities with strictly more than one occurrence in the source article; $S$:
\begin{align*}
P &= \{e\;|\;\text{occurences}(e, S) > 1\}
\intertext{$\forall (e_1, e_2) \in \{P\times{P}\}$ if $e_1 $ is a substring of $e_2$, we call $e_1$ an \textit{alias} of $e_2$. Due to the substring restriction, aliasing is not commutative. Let $A$ be the set of alias pairs $(e_1, e_2)$ in $S$;} 
A &= \{(a, e) \;|\;a \text{ is an alias of } e\}
\intertext{We require the first term of every pair in $A$ to be unique, but there is no such constraint for the second term ($e$); this allows multiple aliases to map to the same entity. For example,} 
 & \{(\text{`Zuckerberg'}, \text{`Mark Zuckerberg'}), (\text{`Zuck'}, \text{`Mark Zuckerberg'})\} 
\intertext{would be unambiguous and therefore valid, but}
 & \{(\text{`Mark'}, \text{`Mark Zuckerberg'}), (\text{`Mark'}, \text{`Zuckerberg'})\}
\intertext{would not be allowed. Let $U$ be the set of unambiguous alias-entity pairs in $S$:}
U &= \{(a, e) \;|\; (a, e)\in{A} \cap \forall(e_1, e_2)\in A, \; a = e_1 \implies e = e_2 \}
\end{align*}

We have a reasonable degree of confidence that for every alias-entity pair $(a, e) \in U$, occurrences of $a$ in the source text can be replaced by $e$, making $e$ a stronger candidate for keyword detection. Pairs in $(a, e) \in A\setminus{U}$, that is, aliases which could map to multiple entities in the source, are disregarded at this stage and left unchanged. Algorithm \ref{alg:u} illustrates how, given a list of entities, the unambiguous pairs can be found deterministically.
\begin{algorithm}
\label{alg:u}
 \caption{Finding unambiguous alias-entity pairs}
 \KwData{$names$: a list of recognised entities}
 \KwResult{$U$: the set of unambiguous pairs in $names$}
 $U \gets$ \{\}\;
 \ForEach{$e_1, e_2$ $\in ($names $\times$ names$)$}{
   \If{len($e_1$) $>$ len($e_2$)}{
   	swap($e_1$, $e_2$) \;
   }
  \If(\tcc*[f]{If $e_1$ is a substring of $e_2$}){$e_1 \in e_2$}{
   	\eIf{$e_1 \notin U$}{
   	  $U$[$e_1$] $\gets e_2$ \tcc*[r]{$(e_1, e_2)$ are a candidate pair}
    }{
      delete $U$[$e_1$] \tcc*[r]{$e_1$ is now ambiguous, so remove it}
    }
   }
 }
\end{algorithm}


\subsubsection{Entity Disambiguation with Knowledge Graph} \label{sec:gkg}

The process described in the previous section is a simplistic approach which only addresses the matching of partial name mentions to full mentions. In comparison, \textit{entity disambiguation} refers to the process of determining the identity of entities in a body of text. Performing this process on the sentence `The UK has voted to leave the EU,' should identify `UK' as `The United Kingdom' and `EU' as `The European Union'. As advanced methods for entity disambiguation are both complex and computationally expensive, we did not implement a formal method for this. 

Instead, the list of recognised entities are queried against Google's publicly accessible Knowledge Graph API, which returns a list of potential results as Schema.org\footnote{http://schema.org is an online hierarchy of types managed by W3C (The World Wide Web Consortium)} types. Knowledge Graph is the service which replaced Freebase in 2015, and currently contains over 70 billion facts \citep{knowledgegraph}.

\cite{EntityDisambiguationForKnowledgeBasePopulation} identify three key challenges in entity linking using knowledge bases; name variations, ambiguity, and absence. Absence describes the lack of a corresponding entry for the entity in the knowledge base, which is not an easily tackled problem. Ambiguity is a consequence of the polysemy of many names and acronyms and requires disambiguation to be performed using more contextual methods. 

Both of these problems are left unsolved in the design and implementation of the system due to scoping constraints, although absence is less of a concern in current events news texts. Na\"{i}ve substring matching however, combined with the use of Knowledge Graph and some empirical parametric estimation, are enough to disambiguate name variations of all forms (partial matches, abbreviations, and acronyms) to a sufficient degree and with surprising accuracy.

The key to using Knowledge Graph for disambiguation lies in its most vaguely defined return value. Each result has a score attributed to it by Knowledge Graph, which is an indicator the strength of the match between the entity and the original query. Results are sorted by descending score; the higher the \texttt{resultScore}, the better the match. Although there is no defined upper limit for this value and no official documentation on how the score is derived, comparing the scores of the top two results for a query can provide a measure of certainty, for all but particularly esoteric or unknown entities.

We specify a threshold $\frac{1}{t}$, which is roughly proportional to the likelihood of accepting a false positive match. Then, if dividing the score of the first result by the score of the second yields a number greater than $\frac{1}{t}$, the match is accepted. Provisionally we set $t=0.5$; a discussion of how this value was derived is provided in the next section.

Using a knowledge base for disambiguation also inadvertently solves another problem we encountered while tokenising and parsing articles, this time as a result the expositional style of journalistic writing. While keywords are typically nouns or noun phrases, they can also appear in the form of denominal adjectives. These adjectives are derived from nouns; e.g. `French' implies `France' might be a keyword. Denominal adjectives are not amenable to traditional stemming or lemmatising, but querying Knowledge Graph for `French' returns a top match of `France' with a \texttt{resultScore} of 432.42807; more than four times larger than the next result.

Given a list of entities $E$, and top two results of a Knowledge Graph query for each $e \in E$; $R_e(1)$ and $R_e(2)$ with scores $S_e(1)$ and $S_e(2)$ respectively, our aim is to return a set of zero or more pairs mapping entities in $E$ to their disambiguated forms;

\begin{align*}
K &= \bigg\{(e, R_e(1))\;|\;\frac{S_e(1)}{S_e(2)} > \frac{1}{t}\bigg\}
\end{align*}

Algorithm \ref{alg:kg} describes this process. For higher values of $t$, the likelihood of accepting a false positive increases, and for lower values, the likelihood of accepting a false negative increases. For corpora which do not contain references to public figures and place names, choice of $t$ should be empirically tuned against the prevalence of the expected entities.\\

\begin{algorithm}
\label{alg:kg}
 \caption{Entity disambiguation with Knowledge Graph}
 \KwData{$names$: a list of recognised entities \\ 
 		\hspace{1.2cm}$t$: the acceptance threshold for results, default = 0.5}
 \KwResult{$K$: a set of disambiguation mappings for elements in $names$}
 $K \gets$ \{\}\;
 $T \gets 1\div{t}$\;
 \ForEach{$e \in names$}{
 	$results \gets$ KnowledgeGraphResults($e$)\;
 	\If{len($results$) $>$ 1}{
	   \If{$results$[0].score $\div$ $results$[1].score $> T$}{
	   		$K[e] = results[0].name$\;
	   }
	}
 }
\end{algorithm}

It is unclear what should become of queries that Knowledge Graph only returns a single result for, but in this implementation they are ignored. Similarly, although Algorithm \ref{alg:kg} terminates with a set of unambiguous mappings, there is still the potential for the system to identify mappings such as:
\begin{align*}
K &= \{(\text{`Zuck'}, \text{`Zuckerberg'}), (\text{`Zuckerberg'}, \text{`Mark Zuckerberg'})\} \\
\text{which simplifies to:}\\
K &= \{(\text{`Zuck'}, \text{`Mark Zuckerberg'}), (\text{`Zuckerberg'}, \text{`Mark Zuckerberg'})\}
\end{align*}
A simple process by which the pairs in $K$ can be reduced as follows:
\begin{align*}
Reduce(\{(e, R_e), (f, R_f)\}) &= \{(e, R_f), (f, R_f)\} \text{ if } R_e = f
\end{align*}
This would require careful removal of any circular aliases; removing cases where
\begin{align*}
K &= \{(e, R_e), (R_e, e)\}
\end{align*}
Pairs in $K$ could then be iterated over, applying this reduction rule until there are no remaining aliases which are themselves aliased to. 

\subsubsection{Determining Optimal Values for $t$}

Empirically, we found the best values for $t$ were in the range $0.35 < t < 0.65$, meaning the system accepts top results which are at least 1.5x-3x higher than the next best candidate. To determine this, we logged all the disambiguation pairs found in 20 articles from the BBC Politics RSS feed, letting $t$ range over \{0.3, 0.5, 0.7, 0.9\}. These pairs were then manually classified as either true or false positives, where a true positive indicates a correctly identified entity, and a false positive is either an incorrectly disambiguated entity or a non-entity which was mistakenly recognised. Figure \ref{fig:dthreshold} shows the results of this investigation.\\

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=\textwidth]{img/implementation/DisambiguationThreshold.pdf}
	\caption{Disambiguation threshold against pairs found and false positives (\%)}
	\label{fig:dthreshold}
\end{figure}

Although a reduction can be seen in false positives for smaller values of $t$, the trend-line illustrates that this is a game of diminishing returns; reducing $t$ from 0.3 to 0.1 only reduces the false positives by 4.34\%, but it leads to 43 fewer pairs being identified\footnote{Since it is both extensive and tangential to the focus of the project, raw data for this table can be found at \url{http://bit.ly/DisambiguationThresholding}.}. 

There is a clear trade-off between the accuracy of the disambiguation and the number of false negatives which are discarded, but the cost of false positives in this case is less than the cost of false negatives. While a false positive could result in unrelated entities appearing as keywords for certain articles, the likelihood of the same false positive appearing with high frequency in enough articles to result in an erroneous metro line is incredibly low. In contrast, the cost of disregarding a false negative could result in articles being left off certain metro lines altogether. It is for this reason that we do not simply choose the value of $t$ which yields the minimum ratio of false positives.

\subsection{Choosing a Term-Weighting Metric for Keyword Ranking}

Given a set of disambiguated entities for every article, the union of which forms a set of candidates keywords or \textit{metro lines} for the collection, we must now determine which keywords are the most relevant. To do this, the two term-weighting methods described in Chapter \ref{c:litreview} will be revisited; tf-idf \citep{tfidf} and tf-pdf \citep{TopicExtractionfromnewsArchiveUsingTFPDFAlgorithm}. Both were implemented in the system so they could be compared.

From an implementation perspective, the main difference between these two algorithms are the level at which they operate. Tf-idf ranks keywords on a per-article basis, returning a vector of an article's keywords and their corresponding score against the background corpus. In contrast, tf-pdf is specified at a corpus level and only returns a single metric for a given word in a corpus; the sum of its significance across the whole corpus. When using tf-pdf, corpora are decomposed into one or more \textit{channels}, which contain articles. In our system, channels can be conveniently defined as RSS feeds from different publishers. 

\citeauthor{TopicExtractionfromnewsArchiveUsingTFPDFAlgorithm} argue that tf-pdf is a more suitable metric for news corpora, because the algorithm discriminates between articles which originated from the same channel and articles in different channels, allowing keywords which are highly (and uniformly) frequent in one channel to be identified as significant in documents which originated from a different channel. Similarly to tf-idf, this process can be used to produce a vector of the highest ranked keywords within a corpus. Tf-idf however requires an additional later level of selection to transform the keyword vectors of each individual article into one global keyword vector of candidate metro lines for the corpus. 

The actual scores attributed to keywords by either algorithm are not of direct importance, since it is only relative scores which are used to construct the keyword vectors. Regardless of which algorithm is used, the advantage to restricting the set of candidates to named entities can be quantified. 
\begin{figure}[htbp!]
	\centering
	\includegraphics[width=\textwidth]{img/implementation/TokensEntities.pdf}
	\caption{Entities as a percentage of Tokens across 40 BBC Politics articles.}
	\label{fig:tokensentities}
\end{figure}

Figure \ref{fig:tokensentities} shows the number of tokens against the percentage of extracted entities for 40 articles from the BBC's Politics RSS Feed.\footnote{http://feeds.bbci.co.uk/news/politics/rss.xml (Accessed: 27/02/2017, Full data in Appendix \ref{tab:entitiestokens})} The interquartile range shows 50\% of articles had entities comprising 5.4\%-8.2\% of their tokens after stop-word removal. With the mean equal to 6.65\%, the implications of this are a search space which can be reduced by a factor of more than ten. Although performance optimisation was not specified in the aims of the project, gains of this nature are still significant.

Once articles have been reduced to vectors of entities, given optional user-specified lists of keywords to either \textit{include} or \textit{ignore}, the system can penalise or boost the scores of articles which mention these topics, which in the case of the \textit{include} list will almost guarantee the keyword is selected as a metro line in the map, as long as it is present in the keyword vectors of at least two articles. 

The function of the \textit{ignore} set is not to blacklist articles which mention certain topics from the map, as it will have no effect on such articles being placed on other metro lines. Instead, it performs domain specific stop-word removal for metro line candidates, preventing non-useful keywords with inevitably high scores (such as `United Kingdom' for a metro map based on a UK news corpus) from being selected. 

During implementation, it was found to be useful to automatically append the names of every news publisher (e.g. `The Guardian') and author in the feeds to the \textit{ignore} set, as well as the names of the feeds themselves. This forces the system to select more specific candidates, even though their scores may be significantly lower.

While implementing the \textit{ignore} functionality the differing results of using tf-idf and tf-pdf on the same corpus became obvious. Because tf-idf penalises words which appear with low but constant frequency across a number of documents, keywords we intuitively would wish to ignore such as `United Kingdom' are not scored as highly as they would be with tf-pdf. 

This is in line with the findings of \citeauthor{TopicExtractionfromnewsArchiveUsingTFPDFAlgorithm}; tf-pdf is more useful for detecting low-frequency keywords which are consistent across many documents. However, this behaviour is exactly the opposite of what we desire from the  metro lines produced by the system; those low-frequency keywords are most likely already known or could be easily inferred by a user reading the map. 

We therefore recommend tf-idf be given preference over tf-pdf in any implementations of metro map systems for news, where the names of more specific themes or topics are being sought. The exception to this is when using a corpus constructed from RSS feeds which share very few topic themes. If this is the case, tf-idf will struggle to extract any topics common enough to form metro lines from, resulting in an overly sparse map. The higher-level topics which are less severely penalised by tf-pdf will provide the most cohesive overview in these instances.

\clearpage

\section{Graph Building} 

The key challenge of the graph building process is generalising the characteristics of a good overview, from both news consumer and metro map user perspectives. It is at this point that a significant difference emerges between this system and the work of \cite{MetroMapsOfScience, GeneratingInformationMaps, InformationCartographyPre}. 

\citeauthor{GeneratingInformationMaps} generated maps in response to a query (such as the name of an event or time) period, meaning the entire map would then be oriented around that query. In contrast, in our system the only query is an implied one; ``What's going on today?" which offers no starting point for choosing metro lines. Instead of a query-based search problem, we have a multi-document summarisation problem. While they were choosing a set of lines to fit a given query, we are concerned with choosing a set of lines which best cover an entire corpus.

\subsection{Extending the Definition of Coverage to Paths}\label{sec:linecoverage}

Our starting point in this section is a corpus of articles and their associated keyword vectors, which form a metro map by \citeauthor{GeneratingInformationMaps}'s definition, but one with too many edges and metro lines to be readable in practice, as Figure \ref{fig:dense} illustrates. Therefore, in order to selectively prune the number of paths to some fixed upper bound, a metric is required to calculate the relative importance of paths within a corpus.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=\textwidth]{img/implementation/datascienceishard.pdf}
	\caption{An unpruned and unusably dense metro map generated by the system}
	\label{fig:dense}
\end{figure}

To do this, we first recall the original definition of document coverage (equation \ref{eqn:doc-coverage} \citep{GeneratingInformationMaps, MetroMapsOfScience, InformationCartographyPre}), where $d$ is a document, $w$ is a keyword, and $\mathcal{W}$ is some normalised measure of term-frequency\footnote{In practice however, we found this normalisation to be unnecessary.
}, such as tf-idf.
\begin{equation}
	cover_{d}(w) : \mathcal{W} \rightarrow [0,1]
	\tag{\ref{eqn:doc-coverage}}
\end{equation}

In order to compare candidate metro lines, we build on this definition of coverage to extend it to paths (Equation \ref{eqn:line-coverage}).

\begin{equation}
	Coverage(\mathcal{P}) = {|\mathcal{P}|}\sum_{d\,\in\,\mathcal{P}}\frac{cover_{d}(\mathcal{P})}{|\{\,p\in{\Pi}\;|\;d \in p,\,p \neq \mathcal{P}\}|}
	\label{eqn:line-coverage}
\end{equation}

Here, the extent to which a path $\mathcal{P}$ covers a document $d$ within a corpus $\mathcal{D}$ is proportional to its tf-idf coverage of that document and inversely proportional to the number of other candidate paths which \textit{also} cover that document. The coverage of the entire corpus by $\mathcal{P}$ is then given the sum of its coverage of the articles along it multiplied by the length of the path, with high coverage being desirable.

It is important to note that although with the final multiplication we show preference to longer metro lines, the objective of the system is not simply to maximise the number of articles which are included on the generated maps. This could easily be achieved by choosing the most nonspecific universal keywords as metro lines, but the resultant lines would be vague and unhelpful to a reader. It is in the nature of summarisation that not all information can be preserved, and in doing this kind of graph selection, the information lost is those articles which don't form links to any significant topics within the corpus.

\subsection{Penalising Affinity} \label{sec:affinity}

The principle of topic connectivity was the basis for choosing the metro map visualisation. Without it, we have simply generated a set of two-dimensional timelines with no contextual links. However, maps which are overly connected will quickly become unusable. In particular, maps where multiple lines runs adjacently through more than two nodes (see figure \ref{fig:lowaffinity}) are a sign of poor line choice. This is not simply hyper-connectivity, but correlation between two or more lines, often resulting from keywords which are semantically close.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=.3\textwidth]{img/implementation/lowaffinity.pdf}
	\caption{An example of three metro lines, all with high affinity}
	\label{fig:lowaffinity}
\end{figure}

We call this function of two lines their \textit{affinity}, and define the affinity of a single line as the sum of its affinities over the set of paths, excluding itself.

\begin{equation}
	Affinity(\mathcal{P}) = \frac{1}{|\mathcal{P}|} \sum_{p\,\in{\,\Pi}} 2^{|\{\,d\,|\,d\,\in\,{\mathcal{P}}\,\cap\,d\,\in\,{p},\,p\,\neq\,\mathcal{P}\}|}
\end{equation}

The affinity of two lines is defined as the number of articles they have in common, raised as a power of two. The effect of the power is illustrated in Figure \ref{fig:abcaffinity}. On the right, line $a$ (orange) which shares one article with line $b$ (green) and another with line $c$ (blue) does not indicate as strong a correlation as on the left, where $a$ shares two articles with $b$ and none with $c$. As with line coverage, we penalise shorter lines.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=.7\textwidth]{img/implementation/abcaffinity.pdf}
	\caption{`Run-on' affinity (left) is penalised more heavily than `one-off' affinity (right)}
	\label{fig:abcaffinity}
\end{figure}

Since metro lines should have both high line coverage and low affinity, a score is computed for every path by dividing the two composite measures (Equation \ref{eq:score}).
\begin{equation}
	LineScore(\mathcal{P}) = \frac{Coverage(\mathcal{P})}{Affinity(\mathcal{P})}
	\label{eq:score}
\end{equation}

Sorting the list of candidates by descending score and filtering out those candidates which contain too few articles to be beneficial, the system can can then take the top $n$ from the range $7 \leq\;n\;\leq 12$ to become the metro lines on the map. Any article which contains the name of a metro line in its keyword vector will be represented as a station on that line, and articles on more metro line than one will be represented as interchanges.

\subsection{Coherence}
The last metric proposed by \citeauthor{GeneratingInformationMaps} is the one which presented the greatest conflict with the predefined scope of the system. For all but the most rapidly unfolding stories, forming a coherent chain of articles on one topic or event is not possible when only considering a single day's news.

\citeauthor{GeneratingInformationMaps} had the advantage of assuming any topic well-known enough to be queried against their system would most likely be made up of a coherent chain of events, such as the Greek debt crisis or Brexit. In contrast, at a daily summarisation level, often no such chains exist; many articles exist in isolation or instead represent the \textit{beginning} of a potential future storyline. Those articles which are part of a long-running chain of events are often still unpredictably published, and these chains of events can go weeks or months without any resolution. Simply altering the parameters of the system to process a week's worth of news rather than a single day's would not solve the problem.

Given a large enough background corpus, it would be possible to link the day's summary back in time to older articles, extending metro lines further back to support the \textit{zoom and filter} or \textit{details-on-demand} processes. This is an area of great potential, as it could provide the story resolution the participants in the \cite{anewmodelfornews} study craved, allowing users to explore further back in time along individual metro lines while still centring itself around the most recent news. Unfortunately however, this is not something we could have achieved within the assigned timescale. 


\subsection{Serialisation}
The intermediate stage of the pipeline between graph building and map drawing is purely an issue of implementation, with few design choices to be made. Starting with the Python representation of the metro map, articles are grouped by metro line to be serialised, along with their metadata and summaries, to JSON. This JSON is then copied into a preexisting templated JavaScript file which is responsible for representing and drawing the graph using D3.js. The contents of the JavaScript file are then embedded as a script in an HTML file, which contains the markup for the page. 

The main advantage to this process is that it results in a single file, meaning the resultant visualisations are as portable, shareable and platform independent as the news they were derived from. A secondary advantage is that the JSON accepted by D3 is standard and could easily be passed a different JavaScript graphing library in the case of a more suitable alternative being written, with no change to the core Python software. The core software itself could also be hosted as a web app which serves the map HTML files, removing the need for any client-side processing or installation.

\clearpage
\section{Map Drawing} \label{sec:drawing}

The metro map layout problem, that is, determining whether an optimal planar embedding exists for a map and a set of hard constraints which include drawing straight octilinear lines, was proven to be NP-Hard by \cite{AutomatedDrawingOfMetroMaps}. The authors present a mixed-integer program (MIP) approach to metro map layouts, which both guarantees an octilinear layout and avoids falling into local maxima (unlike \citeauthor{AutomaticMetroMapLayoutThesis}'s), but not in polynomial time.

At the time of writing, there are no public domain algorithms or open-source libraries for generating automatic metro map layouts, in any language. Those which have been proposed in academia \citep{AutomaticMetroMapLayoutThesis, AutomatedDrawingOfMetroMaps} are too complex to implement given the current timescale, and all require NP-Hard optimisation techniques. To overcome this, a heuristic approach derived from the work of \citep{AutomaticMetroMapLayoutThesis, AutomaticMetroMapLayout, WhichAesthetic} will be detailed.

In this section of the pipeline, we are no longer concerned with the contents of articles or the entities they reference. The map drawing context marks a shift in the terminology from the previous stages of the pipeline to the domain of graph drawing, but many terms have a one-to-one correspondence. A list of definitions is given below, and in addition we recall \citeauthor{GeneratingInformationMaps}'s definition of a metro map from Chapter \ref{c:litreview}, which ties the terms together.

\addtocounter{definition}{-1}
\begin{definition}{}
A metro map $\mathcal{M}$ is a pair $(G, \Pi)$, where $G=(V, E)$ is a directed graph and $\Pi$ is a set of paths, or \textit{metro lines} in $G$. Each $e \in E$ must belong to at least one metro line.
\end{definition}
\vspace{-0.5cm}
\begin{figure}[htbp!]
	\centering
	\includegraphics[width=\textwidth]{img/implementation/mapdefinitions.pdf}
	\vspace{-0.5cm}\caption{Left-to-Right: A station (a), an interchange (b), a metro line (c), a link (d), a terminus (e) and a non-terminus station (f).}
	\label{fig:mapdefinitions}
\end{figure}
\begin{description}[leftmargin=7em,style=nextline]
	\item [Station] A single article $v \in V$, represented by a node in the graph. If $v$ has more than two incident edges, i.e. it represents article which spans more than one topic in the graph, it is called an \textit{interchange}.
	\item [Metro Line] An unbroken named path $p \in \Pi$ which is incident to two or more nodes, represented by a line of single colour. Metro lines are composed of \textit{links}.
	\item [Link] A direct edge $e \in E$ between two nodes, which does not pass through any other node. In contrast to \citeauthor{GeneratingInformationMaps}'s definition, here, links must belong to \textit{exactly one} metro line. The case of two lines running side-by-side through the same two nodes is represented by two distinct links. This difference is due only to an implementation detail of the graphing library chosen.
	\item [Terminus] A node with only one incident edge. This is not the definition of a terminus in the sense of the underlying metaphor, since by our definition, a station which is the terminus for two lines (see Figure \ref{fig:mapdefinitions}.f) is no longer categorised as a terminus at all. Instead, the definition requires that the position of a terminus only determine the position of a single link in the graph.
\end{description}

We now turn our attention to transforming the metro map structure into a planar embedding of stations and the metro lines which connect them. As we have previously observed, not all graph layouts were created equal, and the aesthetic quality of a graph-based visualisations has a notable influence on their usability and ease of interpretation \citep{TheBasisForGraphDrawingAlgorithms, WhichAesthetic, AutomaticMetroMapLayoutThesis, AutomaticMetroMapLayout}. 

In \citeauthor{AutomaticMetroMapLayoutThesis}'s words: ``[The] visualisation process is deeply rooted in the human interpretation of the graph as a structure and as such, the quality of the aesthetics of a particular drawing of a graph are very important.'' \citep[p.24]{AutomaticMetroMapLayoutThesis}

We therefore devote a significant amount of thought to optimising the visual quality of the generated metro maps by revisiting \citeauthor{AutomaticMetroMapLayoutThesis}'s aesthetic criteria for layouts, discussed in Chapter \ref{c:litreview}. We justify which of these apply to our visualisations, and for those which are, we detail the methodology used to implement them.

\subsection{\citeauthor{AutomaticMetroMapLayoutThesis}'s Criteria Metro Map layouts} \label{sec:stottapplication}

The set of rules and criteria defined by \cite{AutomaticMetroMapLayoutThesis} form a complex multicriteria optimisation function. Starting with an initial layout, nodes are embedded on a square grid of finite size, and iteratively moved to maximise the node movement criteria, as long as movement does not violate one of the node movement rules.

It should be noted that the function also applies to the positioning of labels and the maximum of a set of label movement criteria alongside the node movement criteria, but as our stations are unlabelled, these will not be described.

As previously discussed, there are six node movement criteria, which -- using the metro map-specific terms defined above -- are as follows:
\begin{enumerate}
	\item\textbf{Angular Resolution}: Maximise the angle of incident links on the same metro line at each station. As \cite{WhichAesthetic} found satisfying this criteria had no statistical effect on task performance, this will not be a priority\footnote{However, as angular resolution comes as a natural consequence of maximising line straightness, it is not disregarded completely during implementation.}.
	
	\item\textbf{Edge Length}: Keep all links approximately the same length. This is a criterion which can be implemented via a soft constraint using the graphing library, but it will not be considered beyond this.
	
	\item\textbf{Balanced Edge Length}: Keep links incident to a given station approximately the same length. While it would be possible to implement this as an additional constraint, there is no evidence that it would significantly improve usability, so this will not be considered.
	
	\item\textbf{Edge Crossings}: Minimise the number of link crossings. \citeauthor{WhichAesthetic} found this to be the most significant factor in determining task performance, therefore it will be considered.
	
	\item\textbf{Line Straightness}: Maximise the collinearity of links on the same metro line. \citeauthor{WhichAesthetic} found this criterion to be somewhat beneficial to task performance, so this is both a realistic and justified constraint which will be considered. 
	
	\item\textbf{Octilinearity}: Draw links at multiples of 45$^{\circ}$. Although \citeauthor{WhichAesthetic} found orthogonal graphs to be insignificant factors on task performance, there is a strong argument that the familiarity of the metro map metaphor hinges on certain similarities being preserved. Octilinearity is a noticeable design feature in the metro maps of the world, and therefore should be attempted in our visualisations.
\end{enumerate}

Based on the justifications given above, we consider minimising edge crossings to be the highest weighted criterion, followed jointly by maximising octilinearity, then preserving line straightness.

\subsection{Initial Force-Directed Positioning} \label{sec:fdp}

A major difference between the maps generated using this system and maps representing real transit networks is the lack of ``real'' starting positions we have for stations. While the freedom to move points around without having to preserve their relative positioning removes one layout constraint, finding a set of starting positions for the map layout which define a planar embedding with minimal line crossings is non-trivial. 

As \citeauthor{AutomaticMetroMapLayoutThesis} notes, ``This [difficulty] is due to our method being based on optimising an existing layout: if the initial embedding is not adequate, then our method may struggle to produce an acceptable optimisation.''\cite[p.210]{AutomaticMetroMapLayoutThesis}. 

One solution \citeauthor{AutomaticMetroMapLayoutThesis} suggests is to generate positions using a force-directed algorithm; a class of algorithms which simulate the forces of real physical motion to position nodes in a graph, with minimal edge crossings and approximately uniform edge lengths \citep{springembedders}. The drawback to using a force-directed approach is that initial positioning and therefore the final map layout becomes nondeterministic. 

D3 includes an optimised implementation of \possessivecite{forcedirected} force-directed layout algorithm, and provides several parameters which can be used to alter the forces applied to some or all of the nodes in the graph. The two forces we will adjust to optimise the force-direction for metro maps are as follows:
\begin{itemize}
\item\texttt{force.charge([charge])}\footnote{\url{https://github.com/d3/d3-3.x-api-reference/blob/master/Force-Layout.md\#charge}}\par
	\textit{Charge} specifies the force of attraction between nodes, with negative values causing nodes to repel each other. In order to minimise crossings, this should be a large negative value, particularly for nodes which of higher degrees, which will be surrounded by more edges. We therefore set charge to -300 for termini, and -500 for all other stations. 

\item\texttt{force.alpha([alpha])}\footnote{\url{https://github.com/d3/d3-3.x-api-reference/blob/master/Force-Layout.md\#alpha}}\par
	\textit{Alpha} is the graph's cooling parameter, decaying as the simulation converges on a stable layout. As the temperature falls, node movement slows down, and eventually alpha will drop below a threshold which pauses the simulation, preventing further movement. 
\end{itemize}

Unfortunately, the implementation of \texttt{d3.layout.force} lacks any further mechanism for detecting or reducing line crossings due to the computational expense of the best known algorithms (this subproblem is also NP-Hard). Even if line crossings could be detected, it is likely that the topology of certain graphs generated would make it impossible to find a planar embedding with no crossings.

The parameter \texttt{force.alpha} is re-evaluated every time the simulation is advanced by a single step, which happens approximately 60 times per second. Since our aim is to generate an initial layout with minimal line crossings which will result in the best possible map, the simulation can constantly vary \texttt{force.alpha} based on the quality of the map as its stations are repositioned. 

This means that if the stations settle into a map layout with desirably low octilinearity and line straightness, the energy of the graph will be reduced such that it is impossible for the embedding to change. Likewise, if the stations start repelling each other in a way that worsens line straightness and octilinearity, the system will have its kinetic energy increased and will be more likely to move out of the undesirable state.\\

\begin{listing}[htbp!]
\caption{Recalculating \texttt{force.alpha} from \possessivecite{AutomaticMetroMapLayoutThesis} criteria: \texttt{newsgraph.js}}
\label{list:forcealpha}
\inputminted[fontsize=\footnotesize,linenos,numbersep=5pt,firstline=98, lastline=108]{js}{../src/webUtils/newsgraph.js}
\end{listing}

The measure of quality we use to update the kinetic energy of the stations is a combination of the two remaining chosen aesthetic criteria; octilinearity and line straightness, as shown in Listing \ref{list:forcealpha}. Strict definitions for these criteria and the means by which they are calculated are provided in the following two sections. Separate from the map octilinearity criterion but using the same principle is Section \ref{sec:termini}, which describes the repositioning logic for the outer sections of metro maps.

\subsubsection{Octilinearity}

The octilinearity criterion (Equation \ref{eqn:octilinearity} \citep{AutomaticMetroMapLayoutThesis}) penalises lines which are not drawn at multiples of 45$^{\circ}$ angles; the further the gradient of a link in the graph is from a multiple of 45$^{\circ}$, the higher its octilinearity. Lower values are desirable, with zero indicating a graph is entirely octilinear. 

\begin{equation}
	\text{octilinearity}(G) = \sum_{u, v\in E}\bigg|sin\;4\,\bigg(tan^{-1}\frac{|u.y - v.y|}{|u.x - v.x|}\bigg)\bigg|
\label{eqn:octilinearity}
\end{equation}

\begin{figure}[h]
\centering
\begin{minipage}{.4\textwidth}
  \includegraphics[width=.9\linewidth]{img/implementation/octilinearity.pdf}\caption{Octilinearity Calculation\label{fig:octilinearity}}
  \end{minipage}\hspace{0.5cm}\begin{minipage}{.55\textwidth}

The octilinearity of the graph in Figure \ref{fig:octilinearity} (left) can be calculated as follows, taking positions relative to $(0, 0)$ in the bottom left corner. Algorithm \ref{alg:octilinearity} describes this calculation.
$$\begin{aligned}
\text{octilinearity}(a, b) &= |sin\;4\,(tan^{-1}\frac{|1 - 1|}{|4 - 1|})| = 0 \\
\text{octilinearity}(b, c) &= |sin\;4\,(tan^{-1}\frac{|4 - 1|}{|5 - 4|})| = 0.96 \\[0.1cm]
\text{octilinearity}(a, b, c) &= 0 + 0.96 = 0.96
\end{aligned}$$
\end{minipage}
\end{figure}

There is a caveat to this formula which goes unmentioned by \citeauthor{AutomaticMetroMapLayoutThesis}, but which is necessary to emphasise as a detail of implementation. In the case where $u.x - v.x = 0$, we take octilinearity to be zero, as this value indicates a straight vertical line. Otherwise, attempting to calculate the inverse tangent of a fraction whose denominator is zero (though this is mathematically valid, as the principle value of $\tan^{-1}\infty$ is $\frac{\pi}{2}$) would result in undefined behaviour.\\

\begin{algorithm}
\label{alg:octilinearity}
 \caption{Calculating map octilinearity}
 \KwData{$\mathcal{M}$: a metro map $(G=(V, E), \Pi)$}
 \KwResult{$oct \in \mathbb{Q}, 0 \leq oct \leq \frac{24}{25}|E|$: the octilinearity of $\mathcal{M}$}
 $oct \gets 0$ \;
 \ForEach{$e \in E$}{
   $\delta_x \gets |e.source.x - e.target.x|$ \;
   $\delta_y \gets |e.source.y - e.target.y|$ \;
   
   \uIf{$\delta_x = 0 \cup \delta_y = 0$}{
   	 $slope \gets 0$ \tcc*[f]{This avoids a potential division by zero.}}
   	 \Else{
   	  $slope \gets \dfrac{\delta_y}{\delta_x}$ 
    }
   $\theta \gets |sin(4\times{} tan^{-1}(slope))|$ \;
   $oct \gets oct + \theta$ \;
}
\end{algorithm}

\subsubsection{Line Straightness}

The line straightness (Equation \ref{eqn:linestraightness} \citep{AutomaticMetroMapLayoutThesis}) criterion is defined as the sum of every `turn' a metro line makes between its first and last stations, where a turn is measured as the angle between the new link direction and the continuation of the previous direction. The global line straightness is simply the sum of the straightness of every metro line in the map, and as with octilinearity, lower values are preferred.

\begin{equation}
	\text{lineStraigtness}(G) = \sum_{p\,\in{\Pi}}\sum_{u, v\in p} \theta(u, v)
\label{eqn:linestraightness}
\end{equation}

\begin{figure}[h]
\centering
\hspace{-0.5cm}\begin{minipage}{.5\textwidth}
  \centering\includegraphics[width=.8\linewidth]{img/implementation/linestraightness.pdf}\caption{Line Straightness Calculation\label{fig:linestraightness}}
  \end{minipage}\hspace{0.5cm}\begin{minipage}{.5\textwidth}
The line straightness of the graph in Figure \ref{fig:linestraightness} (left) is therefore $\alpha_1 + \alpha_2$, where the values of $\alpha_1$ and $\alpha_2$ can be found geometrically from the direction of the links (Algorithm \ref{alg:ls}).
\end{minipage}
\end{figure}

\begin{algorithm}
\label{alg:ls}
 \caption{Calculating map line straightness}
 \KwData{$\mathcal{M}$: a metro map $(G=(V, E), \Pi)$}
 \KwResult{$ls \in \mathbb{Q}, ls > 0$: the line straightness of $\mathcal{M}$}
 $ls \gets 0$ \;
 \ForEach{$p \in \Pi$}{
 	\tcc{Greedily iterate over all 3-tuples of adjacent stations on $p$}
   \ForEach{$(a, b, c) \in p$}{
   \vspace{0.15cm}
   $v_1 \gets \dfrac{\overrightarrow{ba}}{|\overrightarrow{ba}|}$ ; $v_2 \gets \dfrac{\overrightarrow{bc}}{|\overrightarrow{bc}|}$ ;\tcc*[f]{Normalise the two vectors to unit length}
   \vspace{0.15cm}
   $\theta \gets cos^{-1}(v_1.x\times v_2.x + v_1.y\times v_2.y)$ ;\tcc*[f]{Find the angle between links}
   $ls \gets ls + \theta$ \;
   }
}
\end{algorithm}

Empirically, the best results came from weighting octilinearity and line straightness equally when using them to set \texttt{force.alpha}, but this could be refined through further investigation. 

The actual value assigned to \texttt{force.alpha} is the sum of the natural log of both values, to ensure that small improvements to already low values for either criterion are rewarded. This sum is also divided by a constant, due to the extreme responsiveness of the force-directed algorithm to large values of \texttt{force.alpha}.

\subsection{Heuristic Refinement}

Once all stations have been assigned their starting positions, they are incrementally moved to adjacent intersections, according to a set of conditions for improving the quality of the map. The first step is to discretise the search space for station positions, by \textit{snapping} stations to intersections on a large grid. We found the optimal size for this grid to be in the order of $\frac{n}{4}\times\frac{n}{4}$ for a map containing $n$ stations.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=.7\textwidth]{img/implementation/snap.pdf}
	\caption{An example set of initial moves demonstrating the \textit{snap-to-grid} process}
	\label{fig:snap}
\end{figure}

Figure \ref{fig:snap} shows the \textit{snap-to-grid} process applied to a small example. Stations are `snapped' in order of descending weight (number of incident edges), as typically the more edges a node is connected to, the more difficult it is to place. In the example, this means station $c$ (with weight three) is moved first. There is contention for the second move, as both stations $a$ and $b$ have weight two, so we make an arbitrary decision as to which we move first, and the two moves are shown as one step. The final station to move is station $d$, after which we can mark all stations as successfully placed.

Stations whose nearest intersections are already occupied are left unplaced. This implementation contrasts slightly with \citeauthor{AutomaticMetroMapLayoutThesis}'s approach, where stations are snapped \textit{in order} of Euclidian distance to their nearest intersection, and snapped to the next-nearest if the closest intersection is already occupied. This is because \citeauthor{AutomaticMetroMapLayoutThesis}'s implementation performs poorly for maps with several highly connected nodes, which is a common attribute of metro maps of news, due to the publishing of review articles which often span many of the lines on the map.

To assign the final station positions, our approach now diverges from \citeauthor{AutomaticMetroMapLayoutThesis}'s into a heuristic method which has been refined for metro maps comprising between 10 and 50 stations. It is vital that lines with high affinity have been removed before this stage, or the following process will result in a map where nodes with high weights are occluded.

Although by definition the \textit{snap-to-grid} gives a map near-perfect octilinearity, it can significantly worsen line-straightness and several of the other aesthetic criteria, as shown in Figure \ref{fig:aoct}. It would be preferable to have the circled station repositioned as the transformation shows, even though this sacrifices the uniform edge length of the map. It is therefore necessary to make several passes over each metro line, moving stations to the midpoint of the line between the previous and next neighbouring stations if that line is octilinear.
\begin{figure}[htbp!]
	\centering
	\includegraphics[width=.7\textwidth]{img/implementation/averagingoctilinear.pdf}
	\caption{An octilinear move which compromises the edge-length criterion}
	\label{fig:aoct}
\end{figure}

In addition, a final pass checks for stations which are still unplaced at this point and performs the same midpoint movement regardless of whether the line between the next and previous points is octilinear. This is an attempt to ensure the unplaced point obscures and crosses as few other links as possible, in what is most likely a densely packed area of the map. An example of this process is shown in Figure \ref{fig:nonoct}.
\begin{figure}[htbp!]
	\centering
	\includegraphics[width=.7\textwidth]{img/implementation/averagingnonoctilinear.pdf}
	\caption{A non-octilinear move of an unplaced station}
	\label{fig:nonoct}
\end{figure}

\subsubsection{Octilinearising Terminus Branches} \label{sec:termini}

The three-step process for station movement described above focuses on repositioning the more connected portions of the metro maps generated by the system, which are typically found in the centres of the maps. This process does not help shape \textit{terminus branches}, that is, portions of metro lines which lead to or from its terminating stations and which have no intersections with other metro lines (see the blue metro line in Figure \ref{fig:leaves1}). 

The nature of D3's force-directed algorithm and our lower \texttt{force.charge} rating for terminus stations causes straight lines to curve under the force, resulting in terminus branches being snapped to grid intersections which approximate the curve, as shown in figure \ref{fig:leaves1}, rather than as straight lines.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=.4\textwidth]{img/implementation/leaves1.pdf}
	\caption{A terminus branch containing an unnecessary bend}
	\label{fig:leaves1}
\end{figure}

Terminus branches are often easy to reposition, because there is less contention for space outside the centre portion of the map. They are also not hard to identify; stations which fall between termini and any intersections can be accumulated as part of a branch, and this accumulation continues until an intersection with a different metro line is reached. 
\begin{figure}[htbp!]
	\centering
	\includegraphics[width=.4\textwidth]{img/implementation/leaves2.pdf}
	\caption{The candidate move for the branch in Figure \ref{fig:leaves1}}
	\label{fig:leaves2}
\end{figure}

The repositioning process is illustrated in Figure \ref{fig:leaves2}. The gradient $\alpha$ of line $\overrightarrow{ab}$ is 22.5$^{\circ}$, which, rounded to the nearest octilinear angle is 45$^{\circ}$. The new location for the terminus station $b$ is a line of length $|\overrightarrow{ab}|$, at an angle of 45$^{\circ}$, represented in the figure by the dotted line $\overrightarrow{ab'}$.

The JavaScript implementation of this function shown in Listing \ref{list:octilinearise} works bidirectionally, on branches which both lead \textit{to} a terminus or originate \textit{from} a terminus, according to the chronology of the line. This function performs the exact process shown in Figure \ref{fig:leaves2}, by finding the nearest octilinear angle to the line between the stations \texttt{begin} and \texttt{end}, and returning an object containing the new co-ordinates of the stations. From the co-ordinates of the two branch endpoints, the positions of all the stations between can be interpolated in a separate function.\\

\begin{listing}[htbp!]
\caption{The \texttt{octilineariseLine} function for terminus branches: \texttt{newsgraph.js}}
\label{list:octilinearise}
\inputminted[fontsize=\footnotesize,linenos,numbersep=5pt,firstline=328, lastline=349]{js}{../src/webUtils/newsgraph.js}
\end{listing}
 
Position $b'$ is therefore chosen as a candidate move for station $b$ in Figure \ref{fig:leaves2}. The stations on the branch between the two points are spaced evenly along the line $\overrightarrow{ab'}$, the result of which is shown in Figure \ref{fig:leaves3}.  In the case of an orphaned metro line with no intersections, the result of this process is that the entire metro line will be drawn at an octilinear angle, with its stations evenly spaced between its two termini.

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=.5\textwidth]{img/implementation/leaves3.pdf}
	\caption{The result of octilinearising the terminus branch in Figure \ref{fig:leaves1}}
	\label{fig:leaves3}
\end{figure}

As a final step in the \textit{map drawing} stage, octilinearity and line-straightness are recomputed for the repositioned map, and the number of points left unplaced (i.e. drawn directly on top of another point) are counted. If any of these the numbers are high, the process can be restarted and the randomness of the initial force-direction will result in a different initial embedding. The exact value of `high' is a dependent on the number of articles on the map and has not been sufficiently investigated. Perfect octilinearity and line-straightness are typically only achievable for small or very sparse metro maps, as the more stations a map contains, the more likely it is that an article will be left unplaced.

\section{Summary}

We have given a detailed description of the system's pipeline, covering transformation of news data from XML elements to stations on an interactive metro map. We consider the following processes to be novel or significant successes of development:

\begin{enumerate}
	\item Entity disambiguation through substring matching, denominal adjective stemming, and Google Knowledge Graph result ratios (Section \ref{sec:keys}).
	\item Formalisation of the metrics \textit{Path coverage} and \textit{Line affinity} (Sections \ref{sec:linecoverage} and \ref{sec:affinity}).
	\item The first application of \possessivecite{AutomaticMetroMapLayoutThesis} aesthetic criteria to metro maps representing news corpora (Section \ref{sec:drawing}).
	\item A heuristic function to improve the layout of terminus branches within small ($n<50$) metro maps, by octilinear repositioning of termini (Section \ref{sec:drawing}). 
\end{enumerate}

Several unexpected difficulties arose during the implementation of the system, in particular the tradeoffs involved with the certainty of entity disambiguation, quantifying the logic for determining what makes a ``useful'' metro line, and deciding which metro lines to select between two or more with shared high affinity. 

The most significant challenge of implementation was undoubtedly the process of drawing and refining usable metro maps. The conflict between edge crossings, line-straightness and octilinearity was always at play, and the final algorithm still falls into both local and global minima as it is unable to detect the edge crossings. This section of the pipeline would benefit greatly from further work, as it is a significant area of active research in its own right. 

As the expected datasets for the map are typically small (with fewer than 50 stations), a non-polynomial time algorithm for refining station positions would be feasible in terms of running time, and would doubtless improve the quality of the maps generated.

The results of the process documented in this chapter are illustrated in Figure \ref{fig:final}. This example is available to interact with as \texttt{example-metro-map.html} in the \texttt{src} directory of this project.
\begin{figure}[htbp!]
	\centering
	\includegraphics[width=\textwidth]{img/implementation/final-map.pdf}
	\caption{A metro map generated using the system}
	\label{fig:final}
\end{figure}

Further results obtained by using the system to generate metro maps for various news corpora are presented in the next chapter.

