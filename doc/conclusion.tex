The following chapter outlines the contributions of this work and the limitations of both our system and evaluation. It also describes extensions which could be developed given additional time, and areas which would benefit from further research.

\section{Summary of Contributions}

This dissertation introduced the first application of the metro map metaphor \citep{GeneratingInformationMaps} to news corpora extracted from user-specified RSS feeds, in order to automatically generate structured topic maps based on current events news. These metro maps serve as an effective comprehension aid to young people in our current digital landscape of news overload; a claim which we evaluated empirically through a user study which directly compared our system with a typical RSS feed reader.

Our implementation includes a novel keyword extraction algorithm which performs keyword boosting using a knowledge base for entity disambiguation, and is tuned specifically for extracting entities from current events news. In the subdomain of metro map topology, our method also formalises \textit{Line Coverage} and \textit{Affinity} as an extension to the characteristics defined by \cite{GeneratingInformationMaps}, which can be used to compare the quality of candidate lines during the selection and pruning processes.

Due to the lack of any existing library for positioning or drawing metro maps, further contributions made include recommendations on tuning specific D3.js parameters within a force-directed layout to generate viable starting positions for stations on a metro map, and notes on the implementation of functions to calculate \possessivecite{AutomaticMetroMapLayoutThesis} line straightness and octilinearity criteria in JavaScript. To the best of our knowledge, this is the first application of \citeauthor{AutomaticMetroMapLayoutThesis}'s aesthetic criteria for metro map layout optimisation to non-geospatial maps.

The final contribution of this work is an empirical evaluation of the task performance of young adults using the system. The results of our experiments support our hypothesis that users recall more news topics from a news corpus using our metro maps than after using an unstructured RSS feed reader, and therefore substantiate our recommendations that future efforts to support the contextual linking of news articles should look to visualisation and particularly cartography for direction.

\section{Limitations}

The main limitation of this work is the lack of a formal deterministic algorithm for drawing metro maps, using D3 or otherwise. The approach presented in Section \ref{sec:drawing} is nondeterministic by necessity in order to generate initial starting positions which result in few or no edge crossings, but this process is non-convergent and often results in suboptimal embeddings, the worst cases of which are unusable. 

As a result of this limitation, a significant effort was made to decouple the map drawing process from the other components of the system, such that it could be improved or replaced in future with the only changes required being to the interface at the serialisation stage. While the problem of generating planar embeddings for metro maps is known to be NP-hard, the maps drawn by our system are typically significantly smaller than metro maps which represent real existing transit networks, meaning it would not be unrealistic to apply a non-polynomial time algorithm.

A second important limitation of the system is the inability of the graph formation logic to recognise an overly sparse, overly connected, or overly populated map. All three of these properties are easily addressed in theory; an overly sparse graph should trigger a repeat of the keyword extraction process with a larger keyword vector per article, an overly connected graph should trigger a repeat of the same process with a smaller keyword vector per article, and an overly populated map should use both a smaller keyword vector and have its lowest ranking metro lines removed. However, in the given time, it was not possible to implement this logic in the system, resulting in a need to manually tune parameters such as the above during its operation.

In terms of experimental design, the weaknesses of this work lie in its limited scope. While our user study managed to capture a wide range of news consumption behaviours and attitudes among participants, all participants were students aged 20-23 who were technically proficient. 15 out of the 16 were Computer Science undergraduates, who are typically more familiar with graph theory and therefore more confident interpreting representations of abstract graphs than the general young adult population. This is a limitation which could be addressed by further experiments, as although the system was specifically conceptualised for use by young adults as a result of The \possessivecite{anewmodelfornews} findings, it would have strengthened our conclusions to have evaluated the system in a larger study covering a wider age bracket, and including participants from a non-technical background.

In addition to the lack of participant academic diversity, the scope of our measurement of task performance could also have been extended. The study we conducted was focussed exclusively on recall, but recall is only one of the six factors identified by \cite{VisuelleKommunikation} as being strengthened by the use of visual metaphors; the others being motivation, the formation of new perspectives, support for learning, focusing of attention, and structuring of communication. It could be argued that the act of creating a topic index also evaluated the structure of participants' communication, but this still leaves four factors remaining which we did not evaluate. If our study had been more long-term, we could also have evaluated whether the daily use of metro maps supported participants' learning or increased their motivation to read the news, as these were the two factors identified in Chapter \ref{c:litreview} as the most critical influencers of news fatigue.


\section{Directions for Future Research}

Throughout this work, we have touched on several extensions and adaptations which we envisioned for the system. In summary, these are as follows:
\begin{itemize}
	\item A continually expanding corpus of past articles which can be linked by a contextual knowledge base such as Wikipedia, to extend newly generated metro lines back in time. This would provide context historical to unfolding events, but it would require a significant overhaul to the system, which currently cannot generate metro maps which integrate new content with previously processed articles.
	\item A sophisticated process for performing keyword extraction, again using a contextual knowledge base to disambiguate and identify entities within articles. This linkage could be preserved in the final visualisation, so that entities within summaries in the generated maps contain hyperlinks to their Wikipedia entries. Previous work using Wikipedia for these tasks which we consider to be relevant to this extension includes \citep{LearningToLink} and \citeauthor{Wikify}'s \textit{Wikify} [\citeyear{Wikify}].
	\item The application of the \cite[\citeyear{AutomaticMetroMapLayoutThesis}]{AutomaticMetroMapLayout} multicriteria optimisation function to the metro maps generated by our system. This would require either a JavaScript or Python implementation of the algorithm, which could be optimised by removing all criteria related to station labelling.
\end{itemize}

However, during the design and implementation of the system, the initial stage of our pipeline was the stage which was the most rapidly developed and the least explored in terms of future potential. Therefore, the remainder of this chapter outlines topics we consider to be of interest for future work within this first stage of operation; the acquisition of the news content itself. 

\subsection{Real-Time News Tracking for Automated Collation}

One short-term goal we suggest is to develop the system to a stage where no manual tuning is required to prune the candidate metro lines to a level which strikes a balance between useful and usable. After this, article collation would a simple process to automate, meaning maps could be automatically generated on a periodic basis with no need for user input.

Web services such as News API\footnote{\url{https://newsapi.org}}, which publishes a continual stream of JSON metadata containing live headlines from 72 major worldwide news publishers, are gaining traction as the demand for immediate information increases. Although metro maps have not been designed with live updating in mind, the use of services such as News API could serve as a replacement for specific RSS feeds, removing the major initial parameter of the system.

The key questions posed by the concept of real-time metro maps of news are related to the mechanics of updating, and how stateful the updates should be. Should the publishing of five new headlines trigger a complete regeneration of the map (which would potentially result in a new set of metro lines) or should metro lines be fixed over a specific period of time? Would users want to be able to `pin' metro lines to indicate that the system should consider this an ongoing topic of interest and always choose it as a candidate?

We suspect that there is a limitation on the usefulness of metro maps when their data is continually updating in real-time due to the structural changes which could occur within the maps, but the core idea of dispensing with specific RSS feeds and simply watching major news outlets for trends is an interesting one. While users may find it beneficial to combatting news fatigue to specify exactly where they want their news to originate from or which topics they want to read about, real-time headlines also take the effort out of specifying feeds for users who are indifferent about the sources of the news they consume.

\subsection{Social Media, and the Dangers of the Echo Chamber}

In the current information landscape, it is impossible to discuss real-time content publishing without discussing the impact social networks have on news consumption. The evolution of social networking websites -- in particular, Twitter -- has been instrumental in the erosion of the dichotomy between news media and social media.

The Pew Research Centre reported in 2015 that 61\% of millenials and 51\% of Generation X rank Facebook as their primary source for political news \citep{Facebook}. A risk of this kind of news consumption is that Facebook's newsfeed algorithms determine which news to show users based on their personal preferences and past engagement with articles \citep{FacebookAlgorithm}. Although Facebook and Twitter are often singled out for heavy criticism due to how they filter news content during political campaigns, social media is not entirely to blame; other personal news recommendation systems work in the same manner, albeit more overtly. 

The echo chambers of content which arise from over-accounting for user preferences in news personalisation were coined the \textit{filter bubble} by \cite{TheFilterBubble}, and they result in a deliberate suppression of content which challenges our existing views. A study funded by Facebook itself confirmed this phenomenon, but attributed the filter bubble effect to the promotion of content posted by users' networks of close friends, who often share similar political views \citep{FacebookExposure}.

\begin{center}
\parbox{\textwidth-3cm}{
\textit{``The new generation of Internet filters looks at the things you seem to like - the actual things you've done, or the things people like you like - and tries to extrapolate. They are prediction engines, constantly creating and refining a theory of who you are and what you'll do and want next. Together, these engines create a unique universe of information for each of us - what I've come to call a filter bubble - which fundamentally alters the way we encounter ideas and information.''} \citep[p.29]{TheFilterBubble}}
\end{center}

While our system was developed to run on user-specified feeds, we argue that this kind of explicit content selection is less harmful than the implicit selection which occurs within the filter bubble. As \citeauthor{TheFilterBubble} states, two of the most harmful dynamics of the filter bubble are that we don't choose to enter it, and that we don't always realise we're inside it at all. 
By forcing users to explicitly declare their trusted sources of information, we hope that most users would select content which Facebook's and Twitter's algorithms would deem to be outside of the narrow scope of their filter bubbles. We also expect that users would be less likely to  seek out websites known for producing clickbait articles of their own accord \citep{clickbait}. However, this claim is unverified, and future work is needed to investigate the impact that blindly filtered content recommendations can have on young people. Even in spite of this lack of evidence, we are reluctant to encourage any further work on our system which is based on personalised news recommendations, especially if such recommendations are done without explicitly notifying users. 

In our attempts to address news overload, we cannot compromise on the exposure of young adults in particular to a diverse and balanced range of news content. To do otherwise would be a regression, regardless of innovation behind the approach.

\subsubsection{News Verification}

A second phenomenon which has been blamed on the filter bubble effect is the recent unprecedented rise of \textit{fake news}, that is, deliberately misleading news content which is portrayed as authentic. The internet has always contained misinformation, but it has never been as accessible as it is today. The echo chambers cultivated by social media platforms have been blamed for encouraging speculative rumours based on ``confusion about causation'' \citep[p.1]{TheSpreadingOfMisinformationOnline}.

Although in late 2016 both Facebook and Google took public action against the spread of misinformation by removing websites known for publishing misleading content from their advertising networks, significant damage has already been done. Any content which conflicts with an opposing political ideology can now be dismissed as fake news, even when the content is genuine. Additionally, while Facebook has now partnered with Snopes to develop fake-news-detection software as part of its platform, it relies on users manually flagging suspicious content as potentially fake \citep{FilterBubblesAndFakeNews}, meaning misleading content can still reach users' feeds.

An opportunity presents itself here in the space between content publishers (legitimate or otherwise) and the news feeds of young adults on Twitter and Facebook. If a trusted aggregator were to publish news content on either of these platforms, users could know that they were both receiving content from a wide range of publishing outlets, and that the content itself was genuine. 

Verifiable, up-to-date visualisations which could be embedded in Facebook or Twitter posts would reduce the investment required by users who may otherwise not be willing to split their news consumption between social networks and other services. It is in the bridging of this gap between trusted sources and the social networks where young people consume the majority of their news that we see great potential for the development of NewsGraph and other such platforms. 




